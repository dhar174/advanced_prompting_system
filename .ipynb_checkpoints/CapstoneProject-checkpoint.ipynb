{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cc5a84-9275-4c29-81b1-44d7cad7deae",
   "metadata": {},
   "source": [
    "# Capstone Project: Data Science in Python\n",
    "\n",
    "## Your Objective\n",
    "Apply your knowledge and skills from this course to complete a comprehensive data science project. This project will involve identifying a use case, selecting a dataset, performing ETL (Extract, Transform, Load) and feature engineering, developing and tuning machine learning models, deploying those models, and presenting your findings through clear documentation and storytelling.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Identify a Use Case and Select a Dataset\n",
    "\n",
    "### What You’ll Do:\n",
    "- Choose a project idea from a provided list or propose your own (with approval).\n",
    "- Either use one of the datasets provided or find a dataset to use, exploring resources like Kaggle, UCI Machine Learning Repository, or data.gov.\n",
    "\n",
    "### Suggested Project Ideas:\n",
    "1. **Customer Churn Prediction**: Analyze why customers leave a telecom company.\n",
    "2. **Real Estate Price Prediction**: Predict house prices based on features.\n",
    "3. **Sentiment Analysis**: Evaluate sentiment in product review datasets.\n",
    "4. **Healthcare Analysis**: Predict patient outcomes or identify key risk factors.\n",
    "5. **Retail Analytics**: Build a recommender system for a retail business.\n",
    "\n",
    "### Things to Keep in Mind:\n",
    "- Your problem should be actionable, clearly defined, and feasible within the timeline.\n",
    "- Choose a dataset that’s relevant, manageable in size, and rich enough for analysis.\n",
    "\n",
    "### Your Deliverables:\n",
    "- A well-defined use case.\n",
    "- The dataset you’ll use, along with a rationale for your choice.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Perform ETL and Feature Engineering\n",
    "\n",
    "### What You’ll Do:\n",
    "- Document your ETL process: where your data came from, how you cleaned it, and how you prepared it for analysis.\n",
    "- Engineer features that improve model performance.\n",
    "\n",
    "### Your Deliverables:\n",
    "- A clean and prepared dataset.\n",
    "- A detailed description of your ETL steps (data extraction, cleaning, transformation, and loading).\n",
    "- A list of engineered features and why they’re useful.\n",
    "\n",
    "### Things to Keep in Mind:\n",
    "- Clean, well-prepared data is the foundation of good analysis.\n",
    "- You’ll need to handle missing values, outliers, and categorical data carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Train and Evaluate Your Model\n",
    "\n",
    "### What You’ll Do:\n",
    "- Explore different machine learning techniques, such as regression, classification, and clustering.\n",
    "- Build and evaluate models using Python libraries like Scikit-learn or TensorFlow.\n",
    "\n",
    "### Your Deliverables:\n",
    "- A trained machine learning model.\n",
    "- A comparison of multiple models to select the best one.\n",
    "- An evaluation report with metrics like accuracy, precision, recall, or RMSE.\n",
    "\n",
    "### Things to Keep in Mind:\n",
    "- Focus on improving your models iteratively.\n",
    "- Avoid overfitting and underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Tune and Deploy Your Model\n",
    "\n",
    "### What You’ll Do:\n",
    "- Fine-tune your model’s hyperparameters using tools like GridSearchCV or RandomizedSearchCV.\n",
    "- Deploy your model using frameworks like Streamlit, Flask, or FastAPI. Streamlit is especially beginner-friendly for creating interactive web apps.\n",
    "\n",
    "### Your Deliverables:\n",
    "- A tuned model with optimal hyperparameters.\n",
    "- A deployment-ready script or application that showcases your model.\n",
    "\n",
    "### Things to Keep in Mind:\n",
    "- Simplicity is key in deployment—make your app user-friendly.\n",
    "- Consider real-world challenges like performance and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Document and Present Your Work\n",
    "\n",
    "### What You’ll Do:\n",
    "- Create a final project report that clearly communicates your process and findings.\n",
    "- Prepare a presentation with visualizations to summarize your project.\n",
    "\n",
    "### Your Deliverables:\n",
    "- A detailed project report covering:\n",
    "  - Your problem definition and dataset.\n",
    "  - ETL and feature engineering steps.\n",
    "  - Model training, evaluation, and tuning.\n",
    "  - Key insights and recommendations.\n",
    "- A presentation with visualizations and actionable takeaways.\n",
    "\n",
    "### Things to Keep in Mind:\n",
    "- Make your report clear, concise, and relevant.\n",
    "- Connect your findings back to the original problem.\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric (It's actually a guideline, because its pass or fail. But your project will be evaluated based on the following criteria.)\n",
    "\n",
    "| **Criteria**                 | **Weight** |\n",
    "|------------------------------|------------|\n",
    "| Problem Definition and Dataset Selection | 15%        |\n",
    "| ETL and Feature Engineering  | 20%        |\n",
    "| Model Development and Evaluation | 25% |\n",
    "| Model Tuning and Deployment   | 20%        |\n",
    "| Documentation and Storytelling | 20%        |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "Think creatively and critically—explore unique datasets, try new visualizations, or experiment with hybrid modeling techniques. This capstone project will reinforce your skills and provide a valuable portfolio piece to share with potential employers. Checkpoints, workshops, and feedback sessions will help keep you on track. Dive in, and make this project your own!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84aead-540c-4c4a-9eb3-de47e38315ad",
   "metadata": {},
   "source": [
    "## Run the code cell below to print information for the options for datasets to use for the capstone project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d978bac6-bfce-42a9-9175-35221311c649",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'index.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m      4\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print each row nicely formatted\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'index.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file = \"index.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Print each row nicely formatted\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"\\n\\n Filename: {row['filename']} \\n\")\n",
    "    print(f\"URL: {row['url']} \\n\")\n",
    "    print(f\"Filetype: {row['filetype']}\\n\")\n",
    "    print(f\"Description: {row['description']}\\n\")\n",
    "    print(\"-\" * 40)  # Add a separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364e5f8-5173-4329-8854-5dcd99466152",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb9dba5-0bd2-4464-97b1-0bb207db3370",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "## Detailed Tutorial: Step-by-Step Guide for Completing the Capstone Project\n",
    "\n",
    "Welcome to your Capstone Project! This guide will walk you through each step of the project in detail. Follow these instructions carefully to successfully complete your data science project.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define Your Project Objective\n",
    "\n",
    "**Objective:** Clearly understand what you want to achieve with your project.\n",
    "\n",
    "- **Choose a Project Idea:** Refer to the project ideas listed in previous cells (e.g., Customer Churn Prediction).\n",
    "- **Set Goals:** Define what success looks like. For example, achieving 80% accuracy in predicting customer churn.\n",
    "- **Understand the Problem:** Break down the problem into smaller, manageable parts.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Select and Explore Your Dataset\n",
    "\n",
    "**Objective:** Gather and familiarize yourself with the data you will be working with.\n",
    "\n",
    "- **Load the Data:**\n",
    "    ```python\n",
    "    \n",
    "    df = pd.read_csv('path_to_your_dataset.csv')\n",
    "    ```\n",
    "- **Preview the Data:**\n",
    "    ```python\n",
    "    print(df.head())\n",
    "    print(df.info())\n",
    "    ```\n",
    "- **Understand the Features:** Identify different columns/features and their types (e.g., numerical, categorical).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Data Cleaning\n",
    "\n",
    "**Objective:** Prepare your data for analysis by handling missing values and correcting inconsistencies.\n",
    "\n",
    "- **Identify Missing Values:**\n",
    "    ```python\n",
    "    print(df.isnull().sum())\n",
    "    ```\n",
    "- **Handle Missing Values:**\n",
    "    - **Numerical Features:** Replace missing values with the median or mean.\n",
    "        ```python\n",
    "        df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "        ```\n",
    "    - **Categorical Features:** Replace missing values with the mode or create a new category.\n",
    "        ```python\n",
    "        df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "        ```\n",
    "- **Remove Duplicates:**\n",
    "    ```python\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    ```\n",
    "- **Correct Data Types:**\n",
    "    ```python\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Objective:** Gain insights into the data through visualizations and summary statistics.\n",
    "\n",
    "- **Summary Statistics:**\n",
    "    ```python\n",
    "    print(df.describe())\n",
    "    ```\n",
    "- **Visualize Distributions:**\n",
    "    ```python\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    sns.histplot(df['Age'], kde=True)\n",
    "    plt.show()\n",
    "    ```\n",
    "- **Correlation Analysis:**\n",
    "    ```python\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "    ```\n",
    "- **Analyze Categorical Variables:**\n",
    "    ```python\n",
    "    sns.countplot(x='Gender', data=df)\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Feature Engineering\n",
    "\n",
    "**Objective:** Create new features that can help improve your model's performance.\n",
    "\n",
    "- **Create New Features:**\n",
    "    - **Age Groups:**\n",
    "        ```python\n",
    "        df['AgeGroup'] = pd.cut(df['Age'], bins=[18, 25, 35, 45, 55, 65, 100], labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "        ```\n",
    "    - **Total Spend:**\n",
    "        ```python\n",
    "        df['TotalSpend'] = df['MonthlyCharges'] * df['Tenure']\n",
    "        ```\n",
    "- **Encode Categorical Variables:**\n",
    "    ```python\n",
    "    df = pd.get_dummies(df, columns=['Gender', 'Contract'], drop_first=True)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Split the Data\n",
    "\n",
    "**Objective:** Divide your data into training and testing sets to evaluate your model's performance.\n",
    "\n",
    "- **Import Train-Test Split:**\n",
    "    ```python\n",
    "    ```\n",
    "- **Define Features and Target:**\n",
    "    ```python\n",
    "    X = df.drop('Churn', axis=1)\n",
    "    y = df['Churn']\n",
    "    ```\n",
    "- **Split the Data:**\n",
    "    ```python\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Build and Train Your Model\n",
    "\n",
    "**Objective:** Develop a machine learning model that can predict customer churn.\n",
    "\n",
    "- **Choose a Model:** Start with Logistic Regression.\n",
    "    ```python\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "- **Evaluate Initial Performance:**\n",
    "    ```python\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8: Model Evaluation\n",
    "\n",
    "**Objective:** Assess how well your model is performing using different metrics.\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "    ```python\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    ```\n",
    "- **Classification Report:**\n",
    "    ```python\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ```\n",
    "- **ROC Curve:**\n",
    "    ```python\n",
    "    \n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 9: Hyperparameter Tuning\n",
    "\n",
    "**Objective:** Optimize your model's performance by adjusting its parameters.\n",
    "\n",
    "- **Use GridSearchCV for Logistic Regression:**\n",
    "    ```python\n",
    "    \n",
    "    param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'Best Parameters: {grid.best_params_}')\n",
    "    print(f'Best Score: {grid.best_score_}')\n",
    "    ```\n",
    "- **Retrain with Best Parameters:**\n",
    "    ```python\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 10: Final Model Evaluation\n",
    "\n",
    "**Objective:** Confirm the improvements from hyperparameter tuning.\n",
    "\n",
    "- **Predict and Evaluate:**\n",
    "    ```python\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_best))\n",
    "    ```\n",
    "- **Compare with Previous Metrics:** Ensure that metrics like accuracy, precision, and recall have improved.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 11: Model Deployment\n",
    "\n",
    "**Objective:** Make your model accessible for real-world use.\n",
    "\n",
    "- **Create a Streamlit App:**\n",
    "    ```python\n",
    "    # Install Streamlit if you haven't already\n",
    "    !pip install streamlit\n",
    "    ```\n",
    "\n",
    "- **Write the App Script (`app.py`):**\n",
    "    ```python\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = joblib.load('best_model.pkl')\n",
    "    \n",
    "    st.title('Customer Churn Prediction')\n",
    "    \n",
    "    # Input fields\n",
    "    tenure = st.number_input('Tenure', min_value=0, max_value=100, value=1)\n",
    "    monthly_charges = st.number_input('Monthly Charges', min_value=0.0, max_value=1000.0, value=50.0)\n",
    "    # Add other necessary input fields\n",
    "    \n",
    "    if st.button('Predict'):\n",
    "            input_data = pd.DataFrame({\n",
    "                    'Tenure': [tenure],\n",
    "                    'MonthlyCharges': [monthly_charges],\n",
    "                    # Add other fields\n",
    "            })\n",
    "            prediction = model.predict(input_data)\n",
    "            st.write('Churn Probability:', prediction[0])\n",
    "    ```\n",
    "\n",
    "- **Run the App:**\n",
    "    ```bash\n",
    "    streamlit run app.py\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 12: Documentation and Presentation\n",
    "\n",
    "**Objective:** Communicate your findings and the effectiveness of your model.\n",
    "\n",
    "- **Write a Report:**\n",
    "    - **Introduction:** Explain the problem and objectives.\n",
    "    - **Data Analysis:** Summarize your EDA findings.\n",
    "    - **Modeling:** Describe the models you built and their performance.\n",
    "    - **Conclusion:** Highlight key insights and potential business actions.\n",
    "    \n",
    "- **Create Visualizations:**\n",
    "    - **Feature Importance:** Show which features are most influential.\n",
    "    - **Model Performance:** Include confusion matrices and ROC curves.\n",
    "    \n",
    "- **Prepare a Presentation:**\n",
    "    - **Slides:** Use visuals to tell the story of your project.\n",
    "    - **Key Points:** Focus on problem statement, methodology, results, and recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "- **Stay Organized:** Keep your code clean and well-commented.\n",
    "- **Iterate:** Don’t hesitate to go back and refine earlier steps based on new insights.\n",
    "- **Seek Feedback:** Share your progress with peers or mentors to gain different perspectives.\n",
    "- **Manage Your Time:** Set deadlines for each step to ensure steady progress.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations on completing the Capstone Project! By following these steps, you have developed a comprehensive data science solution that demonstrates your skills and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927387a-6c64-4035-9904-0fe45eb22195",
   "metadata": {},
   "source": [
    "# Detailed Capstone Project Guidelines\n",
    "\n",
    "Below are additional, more detailed guidelines and considerations for each capstone project idea, now that you have selected specific datasets. For each project idea, step-by-step suggestions are provided on how to leverage the procured datasets and what you should pay attention to at each stage of the pipeline. These outlines can be mixed and matched depending on the selected problem domain and dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad2b16-1908-4520-b515-7410565e5105",
   "metadata": {},
   "source": [
    "## Project Idea 1: Customer Churn Prediction (Using: `WA_Fn-UseC_-Telco-Customer-Churn.csv`)\n",
    "\n",
    "### Dataset Characteristics\n",
    "- A CSV file containing demographic and service-related information about customers.\n",
    "- Each row corresponds to a single customer, with attributes like tenure, service subscriptions, contract type, payment method, and whether they have churned or not.\n",
    "\n",
    "### Step-by-Step Specifics\n",
    "#### Step 1: Use Case Identification and Dataset Selection\n",
    "- **Goal**: Predict which customers are likely to stop using the service (churn).\n",
    "- **Dataset Justification**: Well-structured, with target labels for a classification task.\n",
    "- **Success Metrics**: Focus on metrics such as accuracy, precision, recall, F1-score, or AUC (depending on class imbalance).\n",
    "\n",
    "#### Step 2: ETL Process and Feature Engineering\n",
    "- **ETL Tasks**:\n",
    "  - Handle missing values (e.g., replace missing values with median/mode).\n",
    "  - Convert categorical features (e.g., contract type) into numeric form using one-hot encoding.\n",
    "  - Check and handle outliers in continuous variables like monthly charges.\n",
    "- **Feature Engineering**:\n",
    "  - Create tenure-based features (e.g., group customers by tenure ranges).\n",
    "  - Engineer interaction terms, such as combining internet service type and monthly charges.\n",
    "  - Flag payment types that are riskier for churn.\n",
    "\n",
    "#### Step 3: Model Definition, Training, and Evaluation\n",
    "- **Modeling Techniques**: Logistic Regression, Random Forest, Gradient Boosted Trees.\n",
    "- **Evaluation**: Use metrics like recall to focus on identifying churners.\n",
    "\n",
    "#### Step 4: Model Tuning and Deployment\n",
    "- **Hyperparameter Tuning**: Use GridSearchCV for Random Forest depth, `min_samples_split`, etc.\n",
    "- **Deployment**: Build a Streamlit interface to input customer details and predict churn probability.\n",
    "\n",
    "#### Step 5: Documentation and Storytelling\n",
    "- Highlight key features driving churn.\n",
    "- Present actionable business insights for customer retention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18948b5b-5f7e-4d30-9f52-17c0b78e0f66",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Tutorial: Customer Churn Prediction for Beginners\n",
    "\n",
    "Welcome to the **Customer Churn Prediction** tutorial! This guide is designed for absolute beginners and will walk you through each step to help you successfully complete your project. By the end of this tutorial, you'll be able to predict which customers are likely to leave a telecom company using Python and machine learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Customer Churn](#understanding-customer-churn)\n",
    "2. [Setting Up Your Environment](#setting-up-your-environment)\n",
    "3. [Loading the Dataset](#loading-the-dataset)\n",
    "4. [Exploring the Data](#exploring-the-data)\n",
    "5. [Data Cleaning](#data-cleaning)\n",
    "6. [Feature Engineering](#feature-engineering)\n",
    "7. [Splitting the Data](#splitting-the-data)\n",
    "8. [Building the Model](#building-the-model)\n",
    "9. [Evaluating the Model](#evaluating-the-model)\n",
    "10. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "11. [Deploying the Model](#deploying-the-model)\n",
    "12. [Documenting and Presenting Your Work](#documenting-and-presenting-your-work)\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Customer Churn\n",
    "\n",
    "**Customer Churn** refers to the loss of clients or customers who stop using a company's products or services. Predicting churn helps businesses take proactive measures to retain customers, thereby increasing profitability.\n",
    "\n",
    "### Why Predict Churn?\n",
    "\n",
    "- **Cost Efficiency**: Retaining existing customers is cheaper than acquiring new ones.\n",
    "- **Improved Services**: Understanding churn reasons can help improve products/services.\n",
    "- **Revenue Growth**: Reduced churn leads to increased customer lifetime value.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "Before diving into the project, ensure your computer is set up with the necessary tools.\n",
    "\n",
    "### Install Python\n",
    "\n",
    "Download and install the latest version of Python from [python.org](https://www.python.org/downloads/).\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "\n",
    "Open your command prompt or terminal and run:\n",
    "\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "\n",
    "To start Jupyter Notebook, run:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "In a new Jupyter Notebook cell, install the following libraries by running:\n",
    "\n",
    "```python\n",
    "!pip install pandas seaborn scikit-learn matplotlib joblib streamlit\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "\n",
    "In your Jupyter Notebook, import the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import joblib\n",
    "import streamlit as st\n",
    "```\n",
    "\n",
    "### Step 2: Load the Data\n",
    "\n",
    "Assuming your dataset is named `WA_Fn-UseC_-Telco-Customer-Churn.csv` and is in the same directory as your notebook:\n",
    "\n",
    "```python\n",
    "# Load the dataset\n",
    "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exploring the Data\n",
    "\n",
    "Understanding your data is crucial. Let's explore the dataset to get familiar with its structure and contents.\n",
    "\n",
    "### View Dataset Information\n",
    "\n",
    "```python\n",
    "# Get basic information about the dataset\n",
    "df.info()\n",
    "```\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "```python\n",
    "# Get summary statistics for numerical columns\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### Check for Missing Values\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "### Visualize Data Distribution\n",
    "\n",
    "```python\n",
    "# Plot the distribution of numerical features\n",
    "sns.histplot(df['tenure'], kde=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Cleaning your data ensures the quality and reliability of your analysis.\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "Identify columns with missing values and decide how to handle them.\n",
    "\n",
    "```python\n",
    "# Replace spaces with NaN in 'TotalCharges'\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Check again for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop rows with missing 'TotalCharges'\n",
    "df.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "### Convert Data Types\n",
    "\n",
    "Ensure all columns have the correct data types.\n",
    "\n",
    "```python\n",
    "# Convert 'SeniorCitizen' from integer to categorical\n",
    "df['SeniorCitizen'] = df['SeniorCitizen'].astype('object')\n",
    "```\n",
    "\n",
    "### Remove Unnecessary Columns\n",
    "\n",
    "Drop columns that aren't useful for the analysis.\n",
    "\n",
    "```python\n",
    "# Drop 'customerID' as it's not useful for prediction\n",
    "df.drop('customerID', axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Creating new features can help improve your model's performance.\n",
    "\n",
    "### Encode Categorical Variables\n",
    "\n",
    "Convert categorical variables into numerical format.\n",
    "\n",
    "```python\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Display the encoded dataframe\n",
    "df_encoded.head()\n",
    "```\n",
    "\n",
    "### Create New Features\n",
    "\n",
    "Add new features that might be useful for prediction.\n",
    "\n",
    "```python\n",
    "# Example: Total Charges per Month\n",
    "df_encoded['MonthlySpend'] = df_encoded['TotalCharges'] / (df_encoded['tenure'] + 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting the Data\n",
    "\n",
    "Divide the data into training and testing sets to evaluate your model's performance.\n",
    "\n",
    "```python\n",
    "# Define features and target variable\n",
    "X = df_encoded.drop('Churn_Yes', axis=1)\n",
    "y = df_encoded['Churn_Yes']\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "We'll use **Logistic Regression** to predict customer churn.\n",
    "\n",
    "```python\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Assess how well your model performs using various metrics.\n",
    "\n",
    "### Predict on Test Data\n",
    "\n",
    "```python\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Calculate Accuracy\n",
    "\n",
    "```python\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Visualize the performance of the classification model.\n",
    "\n",
    "```python\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "Get detailed performance metrics.\n",
    "\n",
    "```python\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### ROC Curve and AUC\n",
    "\n",
    "Evaluate the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "```python\n",
    "# Predict probabilities\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimize your model by adjusting its parameters to achieve better performance.\n",
    "\n",
    "```python\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f'Best Parameters: {grid.best_params_}')\n",
    "print(f'Best Score: {grid.best_score_:.2f}')\n",
    "```\n",
    "\n",
    "### Retrain with Best Parameters\n",
    "\n",
    "```python\n",
    "# Get the best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Retrain on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Optimized Accuracy: {accuracy_best:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "Make your model accessible through a simple web application using Streamlit.\n",
    "\n",
    "### Save the Model\n",
    "\n",
    "```python\n",
    "# Save the trained model to a file\n",
    "joblib.dump(best_model, 'churn_model.pkl')\n",
    "```\n",
    "\n",
    "### Create a Streamlit App\n",
    "\n",
    "Create a new Python file named `app.py` with the following content:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('churn_model.pkl')\n",
    "\n",
    "st.title('Customer Churn Prediction')\n",
    "\n",
    "# Collect user input\n",
    "tenure = st.number_input('Tenure (months)', min_value=0, max_value=100, value=1)\n",
    "monthly_charges = st.number_input('Monthly Charges', min_value=0.0, max_value=1000.0, value=50.0)\n",
    "total_charges = st.number_input('Total Charges', min_value=0.0, max_value=100000.0, value=50.0)\n",
    "\n",
    "# Add more input fields as needed\n",
    "\n",
    "# Predict churn\n",
    "if st.button('Predict'):\n",
    "    input_data = pd.DataFrame({\n",
    "        'tenure': [tenure],\n",
    "        'MonthlyCharges': [monthly_charges],\n",
    "        'TotalCharges': [total_charges],\n",
    "        # Add other required features here\n",
    "    })\n",
    "    prediction = model.predict(input_data)\n",
    "    if prediction[0] == 1:\n",
    "        st.write('The customer is likely to churn.')\n",
    "    else:\n",
    "        st.write('The customer is likely to stay.')\n",
    "```\n",
    "\n",
    "### Run the Streamlit App\n",
    "\n",
    "In your terminal, run:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "A new browser window will open displaying your web app.\n",
    "\n",
    "---\n",
    "\n",
    "## Documenting and Presenting Your Work\n",
    "\n",
    "Effective documentation and presentation are key to communicating your findings.\n",
    "\n",
    "### Write a Report\n",
    "\n",
    "Include the following sections in your report:\n",
    "\n",
    "- **Introduction**: Explain the problem and objectives.\n",
    "- **Data Exploration**: Summarize your data analysis.\n",
    "- **Data Cleaning and Feature Engineering**: Describe the steps taken to prepare the data.\n",
    "- **Modeling**: Detail the models used and their performance.\n",
    "- **Conclusion**: Highlight key insights and recommendations.\n",
    "\n",
    "### Create Visualizations\n",
    "\n",
    "Use charts and graphs to illustrate your findings.\n",
    "\n",
    "- **Feature Importance**: Show which features are most influential in predicting churn.\n",
    "- **Model Performance**: Include confusion matrices and ROC curves.\n",
    "\n",
    "### Prepare a Presentation\n",
    "\n",
    "Design slides to present your project to stakeholders.\n",
    "\n",
    "- **Slide 1**: Title and objectives.\n",
    "- **Slide 2**: Data overview.\n",
    "- **Slide 3**: Data cleaning and feature engineering.\n",
    "- **Slide 4**: Modeling approach and results.\n",
    "- **Slide 5**: Conclusions and recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "- **Stay Organized**: Keep your code clean and well-commented.\n",
    "- **Understand Your Data**: Spend ample time exploring and understanding your dataset.\n",
    "- **Iterate**: Continuously refine your model based on evaluation metrics.\n",
    "- **Seek Feedback**: Share your progress with peers or mentors for constructive feedback.\n",
    "- **Manage Your Time**: Set deadlines for each project phase to stay on track.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've successfully built a Customer Churn Prediction model. This project not only enhances your machine learning skills but also provides valuable insights that can help businesses retain their customers effectively.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059d2b6-54f4-47f1-b336-15106b4d4d64",
   "metadata": {},
   "source": [
    "## Project Idea 2: Real Estate Price Prediction (Using: `housing_data.csv`)\n",
    "\n",
    "### Dataset Characteristics\n",
    "- A CSV file with housing-related features, including square footage, number of bedrooms/bathrooms, and location factors.\n",
    "\n",
    "### Step-by-Step Specifics\n",
    "#### Step 1: Use Case Identification and Dataset Selection\n",
    "- **Objective**: Predict the sale price of houses.\n",
    "- **Dataset Justification**: Well-suited for regression analysis.\n",
    "- **Success Metrics**: RMSE or MAE.\n",
    "\n",
    "#### Step 2: ETL Process and Feature Engineering\n",
    "- **ETL Tasks**:\n",
    "  - Address missing data in features like lot size or age.\n",
    "  - Normalize continuous features for better model performance.\n",
    "  - Encode categorical variables (e.g., neighborhood) using one-hot encoding.\n",
    "- **Feature Engineering**:\n",
    "  - Create new features like price per square foot.\n",
    "  - Extract location-based features, such as proximity to the city center.\n",
    "\n",
    "#### Step 3: Model Definition, Training, and Evaluation\n",
    "- **Regression Models**: Linear Regression, Random Forest Regressor, Gradient Boosted Regressors.\n",
    "- **Evaluation**: Use RMSE, R², and residual analysis to assess performance.\n",
    "\n",
    "#### Step 4: Model Tuning and Deployment\n",
    "- **Hyperparameter Tuning**: Adjust `n_estimators`, `max_depth`, and similar parameters.\n",
    "- **Deployment**: Create an app to input house features and get price predictions.\n",
    "\n",
    "#### Step 5: Documentation and Storytelling\n",
    "- Highlight key predictors of house price.\n",
    "- Use scatter plots and feature importance charts for insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fb74f-4d43-40f6-9af1-1f7d319e676e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "```markdown\n",
    "# Tutorial: Real Estate Price Prediction for Beginners\n",
    "\n",
    "Welcome to the **Real Estate Price Prediction** tutorial! This guide is designed for absolute beginners and will walk you through each step to help you successfully complete your project. By the end of this tutorial, you'll be able to predict the sale prices of houses using Python and machine learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding the Problem](#understanding-the-problem)\n",
    "2. [Setting Up Your Environment](#setting-up-your-environment)\n",
    "3. [Loading the Dataset](#loading-the-dataset)\n",
    "4. [Exploring the Data](#exploring-the-data)\n",
    "5. [Data Cleaning](#data-cleaning)\n",
    "6. [Feature Engineering](#feature-engineering)\n",
    "7. [Splitting the Data](#splitting-the-data)\n",
    "8. [Building the Model](#building-the-model)\n",
    "9. [Evaluating the Model](#evaluating-the-model)\n",
    "10. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "11. [Deploying the Model](#deploying-the-model)\n",
    "12. [Documenting and Presenting Your Work](#documenting-and-presenting-your-work)\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Problem\n",
    "\n",
    "**Real Estate Price Prediction** involves estimating the sale price of a house based on various features such as size, location, number of bedrooms, and more. Accurate predictions can help buyers make informed decisions and assist sellers in pricing their properties appropriately.\n",
    "\n",
    "### Why Predict House Prices?\n",
    "\n",
    "- **Informed Decisions**: Helps buyers understand the fair market value of properties.\n",
    "- **Investment Analysis**: Assists investors in identifying profitable opportunities.\n",
    "- **Market Trends**: Provides insights into real estate market dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "Before starting, ensure your computer is set up with the necessary tools.\n",
    "\n",
    "### Install Python\n",
    "\n",
    "Download and install the latest version of Python from [python.org](https://www.python.org/downloads/).\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "\n",
    "Open your command prompt or terminal and run:\n",
    "\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "\n",
    "To start Jupyter Notebook, run:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "In a new Jupyter Notebook cell, install the following libraries by running:\n",
    "\n",
    "```python\n",
    "!pip install pandas numpy seaborn scikit-learn matplotlib joblib streamlit\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "\n",
    "In your Jupyter Notebook, import the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import streamlit as st\n",
    "```\n",
    "\n",
    "### Step 2: Load the Data\n",
    "\n",
    "Assuming your dataset is named `housing_data.csv` and is in the same directory as your notebook:\n",
    "\n",
    "```python\n",
    "# Load the dataset\n",
    "df = pd.read_csv('housing_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exploring the Data\n",
    "\n",
    "Understanding your data is crucial. Let's explore the dataset to get familiar with its structure and contents.\n",
    "\n",
    "### View Dataset Information\n",
    "\n",
    "```python\n",
    "# Get basic information about the dataset\n",
    "df.info()\n",
    "```\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "```python\n",
    "# Get summary statistics for numerical columns\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### Check for Missing Values\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "### Visualize Data Distribution\n",
    "\n",
    "```python\n",
    "# Plot the distribution of the target variable 'SalePrice'\n",
    "sns.histplot(df['SalePrice'], kde=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "Understanding how features correlate with the target variable helps in feature selection.\n",
    "\n",
    "```python\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot heatmap of correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Cleaning your data ensures the quality and reliability of your analysis.\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "Identify columns with missing values and decide how to handle them.\n",
    "\n",
    "```python\n",
    "# Identify columns with missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values.sort_values(ascending=False, inplace=True)\n",
    "print(missing_values)\n",
    "```\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "- **Numerical Features:** Replace missing values with the median or mean.\n",
    "- **Categorical Features:** Replace missing values with the mode or create a new category.\n",
    "\n",
    "```python\n",
    "# Example: Fill missing numerical values with median\n",
    "df['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)\n",
    "\n",
    "# Example: Fill missing categorical values with mode\n",
    "df['GarageType'].fillna(df['GarageType'].mode()[0], inplace=True)\n",
    "```\n",
    "\n",
    "### Remove Unnecessary Columns\n",
    "\n",
    "Drop columns that aren't useful for the analysis.\n",
    "\n",
    "```python\n",
    "# Drop 'Id' as it's not useful for prediction\n",
    "df.drop('Id', axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "### Convert Data Types\n",
    "\n",
    "Ensure all columns have the correct data types.\n",
    "\n",
    "```python\n",
    "# Convert 'YearBuilt' and 'YrSold' to integers\n",
    "df['YearBuilt'] = df['YearBuilt'].astype(int)\n",
    "df['YrSold'] = df['YrSold'].astype(int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Creating new features can help improve your model's performance.\n",
    "\n",
    "### Encode Categorical Variables\n",
    "\n",
    "Convert categorical variables into numerical format.\n",
    "\n",
    "```python\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Display the encoded dataframe\n",
    "df_encoded.head()\n",
    "```\n",
    "\n",
    "### Create New Features\n",
    "\n",
    "Add new features that might be useful for prediction.\n",
    "\n",
    "```python\n",
    "# Example: Age of the house at the time of sale\n",
    "df_encoded['HouseAge'] = df_encoded['YrSold'] - df_encoded['YearBuilt']\n",
    "\n",
    "# Example: Total area (sum of all area-related features)\n",
    "df_encoded['TotalArea'] = df_encoded['GrLivArea'] + df_encoded['TotalBsmtSF'] + df_encoded['GarageArea']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting the Data\n",
    "\n",
    "Divide the data into training and testing sets to evaluate your model's performance.\n",
    "\n",
    "### Define Features and Target Variable\n",
    "\n",
    "```python\n",
    "# Define the target variable\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Define feature variables\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "```\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "```python\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "We'll use **Linear Regression** and **Random Forest Regressor** to predict house prices.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "```python\n",
    "# Initialize the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Random Forest Regressor\n",
    "\n",
    "```python\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Assess how well your model performs using various metrics.\n",
    "\n",
    "### Predict on Test Data\n",
    "\n",
    "```python\n",
    "# Predictions using Linear Regression\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Predictions using Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Calculate Evaluation Metrics\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "```python\n",
    "# MAE for Linear Regression\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "print(f'Linear Regression MAE: {mae_lr}')\n",
    "\n",
    "# MAE for Random Forest\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f'Random Forest MAE: {mae_rf}')\n",
    "```\n",
    "\n",
    "#### Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "\n",
    "```python\n",
    "# MSE and RMSE for Linear Regression\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "print(f'Linear Regression RMSE: {rmse_lr}')\n",
    "\n",
    "# MSE and RMSE for Random Forest\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "print(f'Random Forest RMSE: {rmse_rf}')\n",
    "```\n",
    "\n",
    "#### R-squared (R²) Score\n",
    "\n",
    "```python\n",
    "# R² for Linear Regression\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(f'Linear Regression R²: {r2_lr}')\n",
    "\n",
    "# R² for Random Forest\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest R²: {r2_rf}')\n",
    "```\n",
    "\n",
    "### Compare Model Performance\n",
    "\n",
    "Based on the evaluation metrics, determine which model performs better.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimize your model by adjusting its parameters to achieve better performance.\n",
    "\n",
    "### Tune Random Forest Regressor\n",
    "\n",
    "```python\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {-grid_search.best_score_}')\n",
    "```\n",
    "\n",
    "### Retrain with Best Parameters\n",
    "\n",
    "```python\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Retrain on the full training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_best_rf = np.sqrt(mean_squared_error(y_test, y_pred_best_rf))\n",
    "print(f'Optimized Random Forest RMSE: {rmse_best_rf}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "Make your model accessible through a simple web application using Streamlit.\n",
    "\n",
    "### Save the Model\n",
    "\n",
    "```python\n",
    "# Save the trained model to a file\n",
    "joblib.dump(best_rf, 'real_estate_model.pkl')\n",
    "```\n",
    "\n",
    "### Create a Streamlit App\n",
    "\n",
    "Create a new Python file named `app.py` with the following content:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('real_estate_model.pkl')\n",
    "\n",
    "st.title('Real Estate Price Prediction')\n",
    "\n",
    "# Collect user input\n",
    "LotArea = st.number_input('Lot Area (in sq ft)', min_value=0, value=5000)\n",
    "OverallQual = st.selectbox('Overall Quality (1-10)', options=list(range(1, 11)), index=5)\n",
    "GrLivArea = st.number_input('Above Grade Living Area (in sq ft)', min_value=0, value=1500)\n",
    "TotalBsmtSF = st.number_input('Total Basement Area (in sq ft)', min_value=0, value=500)\n",
    "GarageArea = st.number_input('Garage Area (in sq ft)', min_value=0, value=200)\n",
    "\n",
    "# Add more input fields as needed\n",
    "\n",
    "# Predict price\n",
    "if st.button('Predict'):\n",
    "    input_data = pd.DataFrame({\n",
    "        'LotArea': [LotArea],\n",
    "        'OverallQual': [OverallQual],\n",
    "        'GrLivArea': [GrLivArea],\n",
    "        'TotalBsmtSF': [TotalBsmtSF],\n",
    "        'GarageArea': [GarageArea]\n",
    "        # Add other features here\n",
    "    })\n",
    "    prediction = model.predict(input_data)\n",
    "    st.write(f'**Predicted Sale Price:** ${prediction[0]:,.2f}')\n",
    "```\n",
    "\n",
    "### Run the Streamlit App\n",
    "\n",
    "In your terminal, run:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "A new browser window will open displaying your web app.\n",
    "\n",
    "---\n",
    "\n",
    "## Documenting and Presenting Your Work\n",
    "\n",
    "Effective documentation and presentation are key to communicating your findings.\n",
    "\n",
    "### Write a Report\n",
    "\n",
    "Include the following sections in your report:\n",
    "\n",
    "- **Introduction**: Explain the problem and objectives.\n",
    "- **Data Exploration**: Summarize your data analysis.\n",
    "- **Data Cleaning and Feature Engineering**: Describe the steps taken to prepare the data.\n",
    "- **Modeling**: Detail the models used and their performance.\n",
    "- **Conclusion**: Highlight key insights and recommendations.\n",
    "\n",
    "### Create Visualizations\n",
    "\n",
    "Use charts and graphs to illustrate your findings.\n",
    "\n",
    "- **Feature Importance**: Show which features are most influential in predicting house prices.\n",
    "\n",
    "    ```python\n",
    "    # Feature importance for Random Forest\n",
    "    importances = best_rf.feature_importances_\n",
    "    indices = np.argsort(importances)[-10:]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "- **Model Performance**: Include plots like residuals and predicted vs actual values.\n",
    "\n",
    "    ```python\n",
    "    # Predicted vs Actual\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_test, y_pred_best_rf, alpha=0.7)\n",
    "    plt.xlabel('Actual Sale Price')\n",
    "    plt.ylabel('Predicted Sale Price')\n",
    "    plt.title('Actual vs Predicted Sale Price')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "### Prepare a Presentation\n",
    "\n",
    "Design slides to present your project to stakeholders.\n",
    "\n",
    "- **Slide 1**: Title and Objectives\n",
    "- **Slide 2**: Data Overview\n",
    "- **Slide 3**: Data Cleaning and Feature Engineering\n",
    "- **Slide 4**: Modeling Approach and Results\n",
    "- **Slide 5**: Feature Importance and Key Insights\n",
    "- **Slide 6**: Conclusions and Recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "- **Stay Organized**: Keep your code clean and well-commented.\n",
    "- **Understand Your Data**: Spend ample time exploring and understanding your dataset.\n",
    "- **Iterate**: Continuously refine your model based on evaluation metrics.\n",
    "- **Seek Feedback**: Share your progress with peers or mentors for constructive feedback.\n",
    "- **Manage Your Time**: Set deadlines for each project phase to stay on track.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've successfully built a Real Estate Price Prediction model. This project not only enhances your machine learning skills but also provides valuable insights that can help buyers, sellers, and investors make informed decisions in the real estate market.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70f206-a15a-43d5-a9eb-fd698ed189e8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54091284-c6f3-45b1-90e6-6988ee3c7601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project Idea 3: Sentiment Analysis (Using: `reviews.csv`)\n",
    "\n",
    "### Dataset Characteristics\n",
    "- A CSV file with product reviews, containing text data and possibly star ratings.\n",
    "\n",
    "### Step-by-Step Specifics\n",
    "#### Step 1: Use Case Identification and Dataset Selection\n",
    "- **Objective**: Classify reviews as positive, negative, or neutral.\n",
    "- **Dataset Justification**: Well-suited for NLP tasks.\n",
    "- **Success Metrics**: Precision/recall for sentiment classification.\n",
    "\n",
    "#### Step 2: ETL Process and Feature Engineering\n",
    "- **ETL Tasks**:\n",
    "  - Clean text (e.g., remove HTML tags, convert to lowercase).\n",
    "  - Address missing reviews or metadata.\n",
    "- **Feature Engineering**:\n",
    "  - Use TF-IDF vectorization or word embeddings like Word2Vec.\n",
    "  - Extract sentiment lexicon features.\n",
    "\n",
    "#### Step 3: Model Definition, Training, and Evaluation\n",
    "- **Modeling Techniques**: Logistic Regression with TF-IDF, Naive Bayes, or BERT-based models.\n",
    "- **Evaluation**: Use metrics like F1-score and AUC.\n",
    "\n",
    "#### Step 4: Model Tuning and Deployment\n",
    "- **Hyperparameter Tuning**: Adjust parameters like TF-IDF `n-grams`.\n",
    "- **Deployment**: Create a sentiment analysis app to input reviews and predict sentiment.\n",
    "\n",
    "#### Step 5: Documentation and Storytelling\n",
    "- Provide visualizations, such as word clouds.\n",
    "- Highlight business implications, like early identification of negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4975b-f88c-4bc6-a1fc-d5c498f72290",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "```markdown\n",
    "# Tutorial: Sentiment Analysis for Beginners\n",
    "\n",
    "Welcome to the **Sentiment Analysis** tutorial! This guide is designed for absolute beginners and will walk you through each step to help you successfully complete your project. By the end of this tutorial, you'll be able to classify product reviews as positive, negative, or neutral using Python and natural language processing (NLP) techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Sentiment Analysis](#understanding-sentiment-analysis)\n",
    "2. [Setting Up Your Environment](#setting-up-your-environment)\n",
    "3. [Loading the Dataset](#loading-the-dataset)\n",
    "4. [Exploring the Data](#exploring-the-data)\n",
    "5. [Data Cleaning and Preprocessing](#data-cleaning-and-preprocessing)\n",
    "6. [Feature Engineering](#feature-engineering)\n",
    "7. [Splitting the Data](#splitting-the-data)\n",
    "8. [Building the Model](#building-the-model)\n",
    "9. [Evaluating the Model](#evaluating-the-model)\n",
    "10. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "11. [Deploying the Model](#deploying-the-model)\n",
    "12. [Documenting and Presenting Your Work](#documenting-and-presenting-your-work)\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Sentiment Analysis\n",
    "\n",
    "**Sentiment Analysis** is a technique used to determine the emotional tone behind a series of words. It helps in understanding the customer opinions, attitudes, and emotions towards a product, service, or topic.\n",
    "\n",
    "### Why Perform Sentiment Analysis?\n",
    "\n",
    "- **Customer Feedback**: Analyze reviews to gauge customer satisfaction.\n",
    "- **Market Research**: Understand public opinion about products or brands.\n",
    "- **Social Media Monitoring**: Track sentiments around events or trends.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "Before starting, ensure your computer is set up with the necessary tools.\n",
    "\n",
    "### Install Python\n",
    "\n",
    "Download and install the latest version of Python from [python.org](https://www.python.org/downloads/).\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "\n",
    "Open your command prompt or terminal and run:\n",
    "\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "\n",
    "To start Jupyter Notebook, run:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "In a new Jupyter Notebook cell, install the following libraries by running:\n",
    "\n",
    "```python\n",
    "!pip install pandas numpy seaborn scikit-learn matplotlib nltk wordcloud joblib streamlit\n",
    "```\n",
    "\n",
    "**Note**: If you're using a new environment or virtual environment, ensure all libraries are installed there.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "\n",
    "In your Jupyter Notebook, import the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "import streamlit as st\n",
    "```\n",
    "\n",
    "### Step 2: Download NLTK Data\n",
    "\n",
    "Some NLTK functionalities require additional data. Download the stopwords corpus:\n",
    "\n",
    "```python\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "### Step 3: Load the Data\n",
    "\n",
    "Assuming your dataset is named `reviews.csv` and is in the same directory as your notebook:\n",
    "\n",
    "```python\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exploring the Data\n",
    "\n",
    "Understanding your data is crucial. Let's explore the dataset to get familiar with its structure and contents.\n",
    "\n",
    "### View Dataset Information\n",
    "\n",
    "```python\n",
    "# Get basic information about the dataset\n",
    "df.info()\n",
    "```\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "```python\n",
    "# Get summary statistics for numerical columns\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### Check for Missing Values\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "### Sample Distribution\n",
    "\n",
    "```python\n",
    "# Plot the distribution of sentiment classes\n",
    "sns.countplot(x='Sentiment', data=df)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Word Cloud Visualization\n",
    "\n",
    "Visualize the most common words in positive and negative reviews.\n",
    "\n",
    "#### Positive Reviews Word Cloud\n",
    "\n",
    "```python\n",
    "# Combine all positive reviews\n",
    "positive_reviews = ' '.join(df[df['Sentiment'] == 'Positive']['Review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Positive Reviews')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Negative Reviews Word Cloud\n",
    "\n",
    "```python\n",
    "# Combine all negative reviews\n",
    "negative_reviews = ' '.join(df[df['Sentiment'] == 'Negative']['Review'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Preparing your data ensures better model performance.\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "Identify and handle any missing values in the dataset.\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "- **Review Text**: If a review is missing, consider dropping the row.\n",
    "- **Sentiment Label**: If sentiment is missing, drop the row as it's essential for training.\n",
    "\n",
    "```python\n",
    "# Drop rows with missing reviews or sentiments\n",
    "df.dropna(subset=['Review', 'Sentiment'], inplace=True)\n",
    "```\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "Clean the review text to improve model performance.\n",
    "\n",
    "#### Convert Text to Lowercase\n",
    "\n",
    "```python\n",
    "df['Review'] = df['Review'].str.lower()\n",
    "```\n",
    "\n",
    "#### Remove Punctuation and Numbers\n",
    "\n",
    "```python\n",
    "import string\n",
    "\n",
    "def remove_punctuation_numbers(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "\n",
    "df['Review'] = df['Review'].apply(remove_punctuation_numbers)\n",
    "```\n",
    "\n",
    "#### Remove Stopwords\n",
    "\n",
    "Stopwords are common words that may not contribute to the sentiment.\n",
    "\n",
    "```python\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop])\n",
    "\n",
    "df['Review'] = df['Review'].apply(remove_stopwords)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Transform the textual data into numerical features that machine learning models can understand.\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "Convert text data into TF-IDF features.\n",
    "\n",
    "```python\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the review texts\n",
    "X_tfidf = tfidf.fit_transform(df['Review']).toarray()\n",
    "```\n",
    "\n",
    "### Encode Sentiment Labels\n",
    "\n",
    "Convert sentiment labels into numerical format.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize Label Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode sentiments\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting the Data\n",
    "\n",
    "Divide the data into training and testing sets to evaluate your model's performance.\n",
    "\n",
    "```python\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "We'll use **Logistic Regression** to classify the sentiments of the reviews.\n",
    "\n",
    "### Initialize the Model\n",
    "\n",
    "```python\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "```\n",
    "\n",
    "### Train the Model\n",
    "\n",
    "```python\n",
    "# Train the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Assess how well your model performs using various metrics.\n",
    "\n",
    "### Predict on Test Data\n",
    "\n",
    "```python\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "```python\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "```python\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Accuracy Score\n",
    "\n",
    "```python\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimize your model by adjusting its parameters to achieve better performance.\n",
    "\n",
    "### Grid Search for Logistic Regression\n",
    "\n",
    "```python\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f'Best Parameters: {grid.best_params_}')\n",
    "print(f'Best Score: {grid.best_score_:.2f}')\n",
    "```\n",
    "\n",
    "### Retrain with Best Parameters\n",
    "\n",
    "```python\n",
    "# Get the best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Retrain on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Optimized Accuracy: {accuracy_best:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "Make your model accessible through a simple web application using Streamlit.\n",
    "\n",
    "### Save the Model\n",
    "\n",
    "```python\n",
    "# Save the trained model and TF-IDF vectorizer to files\n",
    "joblib.dump(best_model, 'sentiment_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "```\n",
    "\n",
    "### Create a Streamlit App\n",
    "\n",
    "Create a new Python file named `app.py` with the following content:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "model = joblib.load('sentiment_model.pkl')\n",
    "tfidf = joblib.load('tfidf_vectorizer.pkl')\n",
    "le = joblib.load('label_encoder.pkl')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    return text\n",
    "\n",
    "st.title('Sentiment Analysis App')\n",
    "\n",
    "# Collect user input\n",
    "review = st.text_area('Enter a product review:', '')\n",
    "\n",
    "# Predict sentiment\n",
    "if st.button('Predict'):\n",
    "    if review:\n",
    "        processed_review = preprocess(review)\n",
    "        vectorized_review = tfidf.transform([processed_review]).toarray()\n",
    "        prediction = model.predict(vectorized_review)\n",
    "        sentiment = le.inverse_transform(prediction)[0]\n",
    "        st.write(f'**Predicted Sentiment:** {sentiment}')\n",
    "    else:\n",
    "        st.write('Please enter a review to analyze.')\n",
    "```\n",
    "\n",
    "### Run the Streamlit App\n",
    "\n",
    "In your terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "A new browser window will open displaying your web app.\n",
    "\n",
    "---\n",
    "\n",
    "## Documenting and Presenting Your Work\n",
    "\n",
    "Effective documentation and presentation are key to communicating your findings.\n",
    "\n",
    "### Write a Report\n",
    "\n",
    "Include the following sections in your report:\n",
    "\n",
    "- **Introduction**: Explain the problem and objectives.\n",
    "- **Data Exploration**: Summarize your data analysis.\n",
    "- **Data Cleaning and Preprocessing**: Describe the steps taken to prepare the data.\n",
    "- **Feature Engineering**: Detail how you transformed the data.\n",
    "- **Modeling**: Explain the models used and their performance.\n",
    "- **Conclusion**: Highlight key insights and recommendations.\n",
    "\n",
    "### Create Visualizations\n",
    "\n",
    "Use charts and graphs to illustrate your findings.\n",
    "\n",
    "- **Word Clouds**: Show common words in positive and negative reviews.\n",
    "- **Confusion Matrix**: Visualize the model's performance.\n",
    "  \n",
    "  ```python\n",
    "  # Example: Plot confusion matrix\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('Actual')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Model Performance Metrics**: Include accuracy scores and classification reports.\n",
    "\n",
    "### Prepare a Presentation\n",
    "\n",
    "Design slides to present your project to stakeholders.\n",
    "\n",
    "- **Slide 1**: Title and Objectives\n",
    "- **Slide 2**: Data Overview\n",
    "- **Slide 3**: Data Cleaning and Preprocessing\n",
    "- **Slide 4**: Feature Engineering\n",
    "- **Slide 5**: Modeling Approach and Results\n",
    "- **Slide 6**: Confusion Matrix and Performance Metrics\n",
    "- **Slide 7**: Deployment and Application Demo\n",
    "- **Slide 8**: Conclusions and Recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "- **Stay Organized**: Keep your code clean and well-commented.\n",
    "- **Understand Your Data**: Spend ample time exploring and understanding your dataset.\n",
    "- **Iterate**: Continuously refine your model based on evaluation metrics.\n",
    "- **Seek Feedback**: Share your progress with peers or mentors for constructive feedback.\n",
    "- **Manage Your Time**: Set deadlines for each project phase to stay on track.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've successfully built a Sentiment Analysis model. This project not only enhances your natural language processing skills but also provides valuable insights into customer opinions and sentiments.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb5eb8-c399-4177-9e61-6b5cae514272",
   "metadata": {},
   "source": [
    "## Project Idea 4: Healthcare Analysis (Using: `heart_disease` Dataset Folder)\n",
    "\n",
    "### Dataset Characteristics\n",
    "- A set of files with features for diagnosing heart disease presence/absence.\n",
    "\n",
    "### Step-by-Step Specifics\n",
    "#### Step 1: Use Case Identification and Dataset Selection\n",
    "- **Objective**: Predict heart disease or identify risk factors.\n",
    "- **Dataset Justification**: Valuable real-world healthcare problem.\n",
    "- **Success Metrics**: Focus on AUC and recall.\n",
    "\n",
    "#### Step 2: ETL Process and Feature Engineering\n",
    "- **ETL Tasks**:\n",
    "  - Unify data from multiple files.\n",
    "  - Handle missing values and ensure consistent formatting.\n",
    "- **Feature Engineering**:\n",
    "  - Group age and cholesterol into ranges.\n",
    "  - Engineer interaction terms for meaningful combinations.\n",
    "\n",
    "#### Step 3: Model Definition, Training, and Evaluation\n",
    "- **Modeling Techniques**: Logistic Regression, Random Forest, XGBoost.\n",
    "- **Evaluation**: Focus on interpretability and AUC.\n",
    "\n",
    "#### Step 4: Model Tuning and Deployment\n",
    "- **Hyperparameter Tuning**: Use RandomizedSearchCV.\n",
    "- **Deployment**: Build a dashboard for clinicians to input patient metrics.\n",
    "\n",
    "#### Step 5: Documentation and Storytelling\n",
    "- Use bar plots for feature importance and provide actionable recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fb8d2-e23c-444e-bfc7-e353c05bc968",
   "metadata": {},
   "source": [
    "# Working with the `heart_disease` Dataset\n",
    "\n",
    "The `heart_disease` dataset consists of multiple files with different formats (e.g., `.data` files) rather than a single standardized CSV. Follow these steps to work effectively with the data:\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Identify and Use the Appropriate Data Files\n",
    "The UCI Heart Disease dataset includes data from four sources: Cleveland, Hungarian, Switzerland, and the VA Long Beach. The primary \"processed\" files are:\n",
    "\n",
    "- `processed.cleveland.data`\n",
    "- `processed.hungarian.data`\n",
    "- `processed.switzerland.data`\n",
    "- `processed.va.data`\n",
    "\n",
    "These files share a similar format and can be combined into a single dataset. Each file contains rows of patient data and columns corresponding to various medical attributes. Use the `heart-disease.names` file (or UCI documentation) for column details.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Assign Column Names from the Documentation\n",
    "The `heart-disease.names` file or UCI repository documentation lists the attributes. For example, the Cleveland dataset includes 14 attributes:\n",
    "\n",
    "- `age`  \n",
    "- `sex`  \n",
    "- `cp` (chest pain type)  \n",
    "- `trestbps` (resting blood pressure)  \n",
    "- `chol` (serum cholesterol)  \n",
    "- `fbs` (fasting blood sugar)  \n",
    "- `restecg` (resting electrocardiographic results)  \n",
    "- `thalach` (maximum heart rate achieved)  \n",
    "- `exang` (exercise-induced angina)  \n",
    "- `oldpeak` (ST depression induced by exercise relative to rest)  \n",
    "- `slope` (slope of the peak exercise ST segment)  \n",
    "- `ca` (number of major vessels colored by fluoroscopy)  \n",
    "- `thal` (thalassemia status)  \n",
    "- `target` (diagnosis of heart disease: 0 for no disease, 1+ for presence of disease)  \n",
    "\n",
    "Ensure the attribute names and order are consistent across all files.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Loading Each File with pandas\n",
    "The processed files typically use a comma-separated format, with missing values denoted by `\"?\"`. Use the `na_values` parameter to treat `\"?\"` as NaN. For example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Column names from dataset documentation\n",
    "column_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \n",
    "                \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "\n",
    "# Load a single file\n",
    "cleveland_df = pd.read_csv(\"processed.cleveland.data\", \n",
    "                           header=None, \n",
    "                           names=column_names, \n",
    "                           na_values=\"?\")\n",
    "```\n",
    "\n",
    "\n",
    "Step 4: Loading and Combining Multiple Files\n",
    "To combine the four processed datasets:\n",
    "\n",
    "python\n",
    "```hungarian_df = pd.read_csv(\"processed.hungarian.data\", header=None, names=column_names, na_values=\"?\")\n",
    "switzerland_df = pd.read_csv(\"processed.switzerland.data\", header=None, names=column_names, na_values=\"?\")\n",
    "va_df = pd.read_csv(\"processed.va.data\", header=None, names=column_names, na_values=\"?\")\n",
    "\n",
    "# Concatenate all datasets\n",
    "combined_df = pd.concat([cleveland_df, hungarian_df, switzerland_df, va_df], ignore_index=True)```\n",
    "Step 5: Data Cleaning and Verification\n",
    "After loading the data:\n",
    "\n",
    "Handle Missing Values: Decide on imputation strategies for NaN values.\n",
    "Check Data Types: Ensure numeric fields are correctly formatted.\n",
    "Validate the Target Variable: Examine the distribution of target and confirm consistency across datasets.\n",
    "Step 6: Additional Considerations\n",
    "Consult Documentation: Use heart-disease.names for detailed descriptions of attributes.\n",
    "Unify the Structure: Filter out incomplete rows or columns and align the dataset for analysis.\n",
    "Summary\n",
    "Assign column names from the dataset documentation.\n",
    "Load each .data file using pd.read_csv(), treating \"?\" as missing values.\n",
    "Optionally merge the processed datasets into one DataFrame.\n",
    "Perform data cleaning and ensure a consistent structure for analysis.\n",
    "Using header=None ensures pandas doesn’t treat the first row as a header (these files typically lack headers). The result is a clean, labeled DataFrame ready for exploration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa1ac5-a976-4102-854c-043732dd596f",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "```markdown\n",
    "# Tutorial: Heart Disease Prediction for Beginners\n",
    "\n",
    "Welcome to the **Heart Disease Prediction** tutorial! This guide is designed for absolute beginners and will walk you through each step to help you successfully complete your project. By the end of this tutorial, you'll be able to predict the presence of heart disease using Python and machine learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding the Problem](#understanding-the-problem)\n",
    "2. [Setting Up Your Environment](#setting-up-your-environment)\n",
    "3. [Understanding the Dataset](#understanding-the-dataset)\n",
    "4. [Loading and Combining the Data](#loading-and-combining-the-data)\n",
    "5. [Data Cleaning and Verification](#data-cleaning-and-verification)\n",
    "6. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n",
    "7. [Feature Engineering](#feature-engineering)\n",
    "8. [Splitting the Data](#splitting-the-data)\n",
    "9. [Building the Model](#building-the-model)\n",
    "10. [Evaluating the Model](#evaluating-the-model)\n",
    "11. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "12. [Deploying the Model](#deploying-the-model)\n",
    "13. [Documenting and Presenting Your Work](#documenting-and-presenting-your-work)\n",
    "14. [Tips for Success](#tips-for-success)\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Problem\n",
    "\n",
    "**Heart Disease Prediction** involves identifying whether a patient is likely to have heart disease based on various medical attributes. Accurate predictions can assist healthcare professionals in early diagnosis and timely intervention, potentially saving lives.\n",
    "\n",
    "### Why Predict Heart Disease?\n",
    "\n",
    "- **Early Detection**: Helps in identifying high-risk individuals before symptoms appear.\n",
    "- **Resource Allocation**: Assists hospitals in allocating resources effectively.\n",
    "- **Patient Awareness**: Empowers patients with information about their health status.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "Before diving into the project, ensure your computer is equipped with the necessary tools.\n",
    "\n",
    "### Install Python\n",
    "\n",
    "1. **Download Python**: Visit [python.org](https://www.python.org/downloads/) and download the latest version of Python suitable for your operating system.\n",
    "2. **Install Python**: Run the installer and follow the on-screen instructions. Ensure you check the option to add Python to your system PATH during installation.\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "\n",
    "Jupyter Notebook is a powerful tool for interactive coding and documentation.\n",
    "\n",
    "1. Open your command prompt or terminal.\n",
    "2. Run the following command to install Jupyter Notebook:\n",
    "\n",
    "    ```bash\n",
    "    pip install notebook\n",
    "    ```\n",
    "\n",
    "3. To launch Jupyter Notebook, execute:\n",
    "\n",
    "    ```bash\n",
    "    jupyter notebook\n",
    "    ```\n",
    "\n",
    "   A new browser window will open, displaying the Jupyter Notebook interface.\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "You'll need several Python libraries for data manipulation, visualization, and machine learning. Install them by running the following command in a new Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Dataset\n",
    "\n",
    "The `heart_disease` dataset is a collection of medical records from multiple sources, aimed at diagnosing the presence of heart disease in patients.\n",
    "\n",
    "### Dataset Characteristics\n",
    "\n",
    "- **Sources**: Cleveland, Hungarian, Switzerland, and VA Long Beach datasets.\n",
    "- **Features**: Includes attributes like age, sex, chest pain type, resting blood pressure, cholesterol levels, etc.\n",
    "- **Target Variable**: `target` (1 indicates presence of heart disease, 0 indicates absence).\n",
    "\n",
    "---\n",
    "\n",
    "## Loading and Combining the Data\n",
    "\n",
    "Since the dataset comprises multiple `.data` files from different sources, you'll need to load and combine them into a single DataFrame for analysis.\n",
    "\n",
    "### Step 1: Assign Column Names\n",
    "\n",
    "First, define the column names based on the dataset documentation.\n",
    "\n",
    "```python\n",
    "# Column names based on dataset documentation\n",
    "column_names = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Step 2: Load Each File with pandas\n",
    "\n",
    "Use `pandas` to read each `.data` file, treating \"?\" as missing values.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load Cleveland dataset\n",
    "cleveland_df = pd.read_csv(\n",
    "    \"processed.cleveland.data\",\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "# Load Hungarian dataset\n",
    "hungarian_df = pd.read_csv(\n",
    "    \"processed.hungarian.data\",\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "# Load Switzerland dataset\n",
    "switzerland_df = pd.read_csv(\n",
    "    \"processed.switzerland.data\",\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "# Load VA Long Beach dataset\n",
    "va_df = pd.read_csv(\n",
    "    \"processed.va.data\",\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    na_values=\"?\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Step 3: Combine All Datasets\n",
    "\n",
    "Concatenate the individual DataFrames into a single DataFrame for comprehensive analysis.\n",
    "\n",
    "```python\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat(\n",
    "    [cleveland_df, hungarian_df, switzerland_df, va_df],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "combined_df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning and Verification\n",
    "\n",
    "Ensuring data quality is crucial for building an effective model.\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "Identify and address missing values in the dataset.\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "missing_values = combined_df.isnull().sum()\n",
    "print(missing_values)\n",
    "```\n",
    "\n",
    "**Handling Strategies:**\n",
    "\n",
    "- **Numerical Features**: Replace missing values with the median or mean.\n",
    "- **Categorical Features**: Replace missing values with the mode or create a new category.\n",
    "\n",
    "```python\n",
    "# Example: Fill missing numerical values with median\n",
    "combined_df['ca'].fillna(combined_df['ca'].median(), inplace=True)\n",
    "combined_df['thal'].fillna(combined_df['thal'].mode()[0], inplace=True)\n",
    "```\n",
    "\n",
    "### Convert Data Types\n",
    "\n",
    "Ensure all columns have appropriate data types for analysis.\n",
    "\n",
    "```python\n",
    "# Convert relevant columns to integer type\n",
    "combined_df['ca'] = combined_df['ca'].astype(int)\n",
    "combined_df['thal'] = combined_df['thal'].astype(int)\n",
    "```\n",
    "\n",
    "### Validate the Target Variable\n",
    "\n",
    "Examine the distribution of the target variable to understand the balance of classes.\n",
    "\n",
    "```python\n",
    "# Distribution of target variable\n",
    "combined_df['target'].value_counts()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Gain insights into the dataset through visualization and statistical analysis.\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "Understand the central tendency and dispersion of numerical features.\n",
    "\n",
    "```python\n",
    "# Summary statistics for numerical columns\n",
    "combined_df.describe()\n",
    "```\n",
    "\n",
    "### Visualize Feature Distributions\n",
    "\n",
    "Use histograms and boxplots to visualize the distribution of key features.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram of age\n",
    "sns.histplot(combined_df['age'], kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of cholesterol levels\n",
    "sns.boxplot(x='target', y='chol', data=combined_df)\n",
    "plt.title('Cholesterol Levels by Heart Disease Status')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "Identify how features correlate with the target variable.\n",
    "\n",
    "```python\n",
    "# Correlation matrix\n",
    "corr_matrix = combined_df.corr()\n",
    "\n",
    "# Heatmap of correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Transform and create new features to improve model performance.\n",
    "\n",
    "### Encode Categorical Variables\n",
    "\n",
    "Convert categorical variables into numerical format using one-hot encoding.\n",
    "\n",
    "```python\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "combined_encoded = pd.get_dummies(combined_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Display the first few rows of the encoded DataFrame\n",
    "combined_encoded.head()\n",
    "```\n",
    "\n",
    "### Create New Features\n",
    "\n",
    "Develop additional features that might enhance the model's predictive power.\n",
    "\n",
    "```python\n",
    "# Example: Age groups\n",
    "def age_group(age):\n",
    "    if age < 40:\n",
    "        return 'Young'\n",
    "    elif age < 60:\n",
    "        return 'Middle-aged'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "combined_encoded['age_group'] = combined_encoded['age'].apply(age_group)\n",
    "\n",
    "# One-hot encode the new age_group feature\n",
    "combined_encoded = pd.get_dummies(combined_encoded, columns=['age_group'], drop_first=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting the Data\n",
    "\n",
    "Divide the dataset into training and testing sets to evaluate your model's performance.\n",
    "\n",
    "### Define Features and Target Variable\n",
    "\n",
    "```python\n",
    "# Define the target variable\n",
    "y = combined_encoded['target']\n",
    "\n",
    "# Define feature variables by dropping the target column\n",
    "X = combined_encoded.drop('target', axis=1)\n",
    "```\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "Use `train_test_split` to create training and testing datasets.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "We'll use **Logistic Regression** and **Random Forest Classifier** to predict heart disease presence.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Random Forest Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Assess how well your models perform using various metrics.\n",
    "\n",
    "### Predict on Test Data\n",
    "\n",
    "```python\n",
    "# Logistic Regression predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Random Forest predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "Understand precision, recall, f1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Logistic Regression Classification Report\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Random Forest Classification Report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Visualize the performance of your models.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Random Forest Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Accuracy Score\n",
    "\n",
    "Measure the overall correctness of the model.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression Accuracy\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f'Logistic Regression Accuracy: {accuracy_lr:.2f}')\n",
    "\n",
    "# Random Forest Accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimize your models by adjusting their parameters to achieve better performance.\n",
    "\n",
    "### Grid Search for Random Forest\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
    "```\n",
    "\n",
    "### Retrain with Best Parameters\n",
    "\n",
    "```python\n",
    "# Get the best estimator\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Retrain the model on the full training data\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "print(f'Optimized Random Forest Accuracy: {accuracy_best_rf:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "Make your model accessible for real-world use through a simple web application.\n",
    "\n",
    "### Save the Trained Model\n",
    "\n",
    "Use `joblib` to save your trained model for later use.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save the best Random Forest model to a file\n",
    "joblib.dump(best_rf_model, 'best_random_forest_model.pkl')\n",
    "```\n",
    "\n",
    "### Create a Streamlit App\n",
    "\n",
    "Streamlit allows you to build interactive web applications for your models.\n",
    "\n",
    "1. **Install Streamlit**:\n",
    "\n",
    "    ```bash\n",
    "    pip install streamlit\n",
    "    ```\n",
    "\n",
    "2. **Create a New Python File**:\n",
    "\n",
    "    Create a file named `app.py` and add the following content:\n",
    "\n",
    "    ```python\n",
    "    import streamlit as st\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load('best_random_forest_model.pkl')\n",
    "\n",
    "    st.title('Heart Disease Prediction App')\n",
    "\n",
    "    # Collect user input\n",
    "    age = st.number_input('Age', min_value=1, max_value=120, value=30)\n",
    "    sex = st.selectbox('Sex', ('Male', 'Female'))\n",
    "    cp = st.selectbox('Chest Pain Type', ('Typical Angina', 'Atypical Angina', 'Non-anginal Pain', 'Asymptomatic'))\n",
    "    trestbps = st.number_input('Resting Blood Pressure', min_value=80, max_value=200, value=120)\n",
    "    chol = st.number_input('Cholesterol', min_value=100, max_value=600, value=200)\n",
    "    fbs = st.selectbox('Fasting Blood Sugar > 120 mg/dl', ('Yes', 'No'))\n",
    "    restecg = st.selectbox('Resting ECG', ('Normal', 'Having ST-T wave abnormality', 'Showing probable or definite left ventricular hypertrophy'))\n",
    "    thalach = st.number_input('Max Heart Rate Achieved', min_value=60, max_value=220, value=150)\n",
    "    exang = st.selectbox('Exercise Induced Angina', ('Yes', 'No'))\n",
    "    oldpeak = st.number_input('ST Depression Induced by Exercise', min_value=0.0, max_value=10.0, value=1.0)\n",
    "    slope = st.selectbox('Slope of the Peak Exercise ST Segment', ('Up', 'Flat', 'Down'))\n",
    "    ca = st.number_input('Number of Major Vessels Colored by Fluoroscopy', min_value=0, max_value=4, value=0)\n",
    "    thal = st.selectbox('Thalassemia', ('Normal', 'Fixed Defect', 'Reversable Defect'))\n",
    "\n",
    "    # Prepare the input data for prediction\n",
    "    input_data = pd.DataFrame({\n",
    "        'age': [age],\n",
    "        'sex': [1 if sex == 'Male' else 0],\n",
    "        'trestbps': [trestbps],\n",
    "        'chol': [chol],\n",
    "        'fbs': [1 if fbs == 'Yes' else 0],\n",
    "        'thalach': [thalach],\n",
    "        'exang': [1 if exang == 'Yes' else 0],\n",
    "        'oldpeak': [oldpeak],\n",
    "        'ca': [ca]\n",
    "    })\n",
    "\n",
    "    # Encode categorical variables\n",
    "    cp_mapping = {'Typical Angina': 0, 'Atypical Angina': 1, 'Non-anginal Pain': 2, 'Asymptomatic': 3}\n",
    "    restecg_mapping = {'Normal': 0, 'Having ST-T wave abnormality': 1, 'Showing probable or definite left ventricular hypertrophy': 2}\n",
    "    slope_mapping = {'Up': 0, 'Flat': 1, 'Down': 2}\n",
    "    thal_mapping = {'Normal': 0, 'Fixed Defect': 1, 'Reversable Defect': 2}\n",
    "\n",
    "    input_data['cp'] = cp_mapping[cp]\n",
    "    input_data['restecg'] = restecg_mapping[restecg]\n",
    "    input_data['slope'] = slope_mapping[slope]\n",
    "    input_data['thal'] = thal_mapping[thal]\n",
    "\n",
    "    # Predict the outcome\n",
    "    if st.button('Predict'):\n",
    "        prediction = model.predict(input_data)\n",
    "        if prediction[0] == 1:\n",
    "            st.error('The model predicts that you have heart disease.')\n",
    "        else:\n",
    "            st.success('The model predicts that you do not have heart disease.')\n",
    "    ```\n",
    "\n",
    "3. **Run the Streamlit App**:\n",
    "\n",
    "    In your terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    streamlit run app.py\n",
    "    ```\n",
    "\n",
    "    A new browser window will open displaying your web application.\n",
    "\n",
    "---\n",
    "\n",
    "## Documenting and Presenting Your Work\n",
    "\n",
    "Effective documentation and presentation are crucial for communicating your findings.\n",
    "\n",
    "### Write a Report\n",
    "\n",
    "Include the following sections in your report:\n",
    "\n",
    "- **Introduction**: Explain the problem and objectives.\n",
    "- **Data Exploration**: Summarize your data analysis.\n",
    "- **Data Cleaning and Preprocessing**: Describe the steps taken to prepare the data.\n",
    "- **Feature Engineering**: Detail how you transformed the data.\n",
    "- **Modeling**: Explain the models used and their performance.\n",
    "- **Conclusion**: Highlight key insights and recommendations.\n",
    "\n",
    "### Create Visualizations\n",
    "\n",
    "Use charts and graphs to illustrate your findings.\n",
    "\n",
    "- **Correlation Matrix**: Shows how features are related to each other.\n",
    "\n",
    "    ```python\n",
    "    # Example: Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "- **Feature Importance**: Displays which features are most influential in predicting heart disease.\n",
    "\n",
    "    ```python\n",
    "    # Feature Importance for Random Forest\n",
    "    importances = best_rf_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=feature_importances[:10], y=feature_importances.index[:10])\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "### Prepare a Presentation\n",
    "\n",
    "Design slides to present your project to stakeholders.\n",
    "\n",
    "- **Slide 1**: Title and Objectives\n",
    "- **Slide 2**: Data Overview\n",
    "- **Slide 3**: Data Cleaning and Preprocessing\n",
    "- **Slide 4**: Feature Engineering\n",
    "- **Slide 5**: Modeling Approach and Results\n",
    "- **Slide 6**: Confusion Matrix and Performance Metrics\n",
    "- **Slide 7**: Feature Importance\n",
    "- **Slide 8**: Deployment and Application Demo\n",
    "- **Slide 9**: Conclusions and Recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "- **Stay Organized**: Keep your code clean and well-commented.\n",
    "- **Understand Your Data**: Spend ample time exploring and understanding your dataset.\n",
    "- **Iterate**: Continuously refine your model based on evaluation metrics.\n",
    "- **Seek Feedback**: Share your progress with peers or mentors for constructive feedback.\n",
    "- **Manage Your Time**: Set deadlines for each project phase to stay on track.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've successfully built a Heart Disease Prediction model. This project not only enhances your machine learning skills but also provides valuable insights into healthcare data analysis.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459905c4-67db-471e-93b3-b9463754763a",
   "metadata": {},
   "source": [
    "## Project Idea 5: Retail Analytics (Using: `online_retail.xlsx`)\n",
    "\n",
    "### Dataset Characteristics\n",
    "- An Excel file with transaction-level data: invoices, product codes, customer IDs, and purchase details.\n",
    "\n",
    "### Step-by-Step Specifics\n",
    "#### Step 1: Use Case Identification and Dataset Selection\n",
    "- **Objective**: Build a recommendation system or predict repeat purchases.\n",
    "- **Dataset Justification**: Rich transactional data for analyzing customer behavior.\n",
    "- **Success Metrics**: Precision@k for recommendations.\n",
    "\n",
    "#### Step 2: ETL Process and Feature Engineering\n",
    "- **ETL Tasks**:\n",
    "  - Clean Excel data and address missing Customer IDs.\n",
    "  - Handle outliers like negative quantities.\n",
    "- **Feature Engineering**:\n",
    "  - Compute RFM (Recency, Frequency, Monetary) scores.\n",
    "  - Aggregate product-level data to customer-level features.\n",
    "\n",
    "#### Step 3: Model Definition, Training, and Evaluation\n",
    "- **Techniques**: Collaborative filtering, classification for reorder prediction.\n",
    "- **Evaluation**: Use metrics like NDCG for recommendations.\n",
    "\n",
    "#### Step 4: Model Tuning and Deployment\n",
    "- **Hyperparameter Tuning**: Adjust latent factors in matrix factorization.\n",
    "- **Deployment**: Create an app to recommend products for a given customer ID.\n",
    "\n",
    "#### Step 5: Documentation and Storytelling\n",
    "- Use visualizations to highlight customer segments and their behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb452868-ea17-4e99-938f-23d73b087d62",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "```markdown\n",
    "# Tutorial: Retail Analytics for Beginners\n",
    "\n",
    "Welcome to the **Retail Analytics** tutorial! This guide is designed for absolute beginners and will walk you through each step to help you successfully complete your project. By the end of this tutorial, you'll be able to analyze retail transaction data, build a recommendation system, or predict repeat purchases using Python and data analysis techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Retail Analytics](#introduction-to-retail-analytics)\n",
    "2. [Setting Up Your Environment](#setting-up-your-environment)\n",
    "3. [Loading the Dataset](#loading-the-dataset)\n",
    "4. [Exploring the Data](#exploring-the-data)\n",
    "5. [Data Cleaning and Preprocessing](#data-cleaning-and-preprocessing)\n",
    "6. [Feature Engineering](#feature-engineering)\n",
    "7. [Splitting the Data](#splitting-the-data)\n",
    "8. [Building the Model](#building-the-model)\n",
    "9. [Evaluating the Model](#evaluating-the-model)\n",
    "10. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "11. [Deploying the Model](#deploying-the-model)\n",
    "12. [Documenting and Presenting Your Work](#documenting-and-presenting-your-work)\n",
    "13. [Tips for Success](#tips-for-success)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Retail Analytics\n",
    "\n",
    "**Retail Analytics** involves analyzing data related to sales, customers, and operations to make informed business decisions. This tutorial focuses on leveraging transactional data from an online retail store to build models that can recommend products or predict repeat purchases.\n",
    "\n",
    "### Why Perform Retail Analytics?\n",
    "\n",
    "- **Improve Customer Experience**: Personalize recommendations to enhance customer satisfaction.\n",
    "- **Increase Sales**: Targeted marketing strategies can lead to higher conversion rates.\n",
    "- **Optimize Inventory**: Understand purchasing patterns to manage stock levels effectively.\n",
    "- **Reduce Churn**: Identify factors that lead to repeat purchases and customer retention.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "Before diving into the project, ensure your computer is equipped with the necessary tools.\n",
    "\n",
    "### Install Python\n",
    "\n",
    "1. **Download Python**: Visit [python.org](https://www.python.org/downloads/) and download the latest version of Python suitable for your operating system.\n",
    "2. **Install Python**: Run the installer and follow the on-screen instructions. Ensure you check the option to add Python to your system PATH during installation.\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "\n",
    "Jupyter Notebook is a powerful tool for interactive coding and documentation.\n",
    "\n",
    "1. Open your command prompt or terminal.\n",
    "2. Run the following command to install Jupyter Notebook:\n",
    "\n",
    "    ```bash\n",
    "    pip install notebook\n",
    "    ```\n",
    "\n",
    "3. To launch Jupyter Notebook, execute:\n",
    "\n",
    "    ```bash\n",
    "    jupyter notebook\n",
    "    ```\n",
    "\n",
    "    A new browser window will open, displaying the Jupyter Notebook interface.\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "You'll need several Python libraries for data manipulation, visualization, and machine learning. Install them by running the following command in a new Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "The `online_retail.xlsx` dataset contains transaction-level data, including invoices, product codes, customer IDs, and purchase details.\n",
    "\n",
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "\n",
    "### Step 2: Load the Excel File\n",
    "\n",
    "Use `pandas` to read the Excel file. Ensure the file is in your working directory or provide the correct path.\n",
    "\n",
    "```python\n",
    "# Load the dataset\n",
    "df = pd.read_excel('online_retail.xlsx')\n",
    "```\n",
    "\n",
    "### Step 3: Explore the DataFrame\n",
    "\n",
    "View the first few rows to understand the structure of the data.\n",
    "\n",
    "```python\n",
    "# Display the first five rows\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exploring the Data\n",
    "\n",
    "Understanding your data is crucial for effective analysis.\n",
    "\n",
    "### Check Data Types and Missing Values\n",
    "\n",
    "```python\n",
    "# Check data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "```\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "Get a statistical overview of the numerical features.\n",
    "\n",
    "```python\n",
    "# Summary statistics\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### Visualize Sales Distribution\n",
    "\n",
    "Plot the distribution of sales to identify patterns or outliers.\n",
    "\n",
    "```python\n",
    "# Histogram of UnitPrice\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['UnitPrice'], bins=50, kde=True)\n",
    "plt.title('Distribution of Unit Price')\n",
    "plt.xlabel('Unit Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Ensure the data is clean and ready for analysis.\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "Identify and address missing Customer IDs, which are essential for customer-based analysis.\n",
    "\n",
    "```python\n",
    "# Drop rows with missing CustomerID\n",
    "df_clean = df.dropna(subset=['CustomerID'])\n",
    "```\n",
    "\n",
    "### Remove Negative Quantities\n",
    "\n",
    "Negative quantities may indicate returns; depending on the analysis, you might want to remove or handle them differently.\n",
    "\n",
    "```python\n",
    "# Remove rows with negative Quantity\n",
    "df_clean = df_clean[df_clean['Quantity'] > 0]\n",
    "```\n",
    "\n",
    "### Convert Data Types\n",
    "\n",
    "Ensure numerical columns have appropriate data types.\n",
    "\n",
    "```python\n",
    "# Convert InvoiceDate to datetime\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "```\n",
    "\n",
    "### Remove Duplicates\n",
    "\n",
    "Check and remove any duplicate records.\n",
    "\n",
    "```python\n",
    "# Remove duplicate rows\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Create new features that can enhance model performance.\n",
    "\n",
    "### Compute Total Price\n",
    "\n",
    "Calculate the total price for each transaction.\n",
    "\n",
    "```python\n",
    "# Total price per transaction\n",
    "df_clean['TotalPrice'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "```\n",
    "\n",
    "### Create RFM Features\n",
    "\n",
    "RFM stands for Recency, Frequency, and Monetary value, which are key indicators of customer behavior.\n",
    "\n",
    "```python\n",
    "import datetime as dt\n",
    "\n",
    "# Define today's date for recency calculation\n",
    "today = dt.datetime(2011, 12, 10)  # Assuming the dataset ends on this date\n",
    "\n",
    "# Group by CustomerID and compute RFM metrics\n",
    "rfm = df_clean.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (today - x.max()).days,\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalPrice': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "```\n",
    "\n",
    "### Handle Outliers in Monetary Value\n",
    "\n",
    "Cap the monetary value to reduce the effect of outliers.\n",
    "\n",
    "```python\n",
    "# Cap Monetary at the 95th percentile\n",
    "max_monetary = rfm['Monetary'].quantile(0.95)\n",
    "rfm = rfm[rfm['Monetary'] <= max_monetary]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting the Data\n",
    "\n",
    "Divide the dataset into training and testing sets for model evaluation.\n",
    "\n",
    "### Define Features and Target Variable\n",
    "\n",
    "For prediction tasks, define what you're trying to predict. For example, predicting repeat purchases based on RFM scores.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define target variable: Repeat purchase (Frequency > 1)\n",
    "rfm['Repeat'] = rfm['Frequency'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "# Features and target\n",
    "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "y = rfm['Repeat']\n",
    "```\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "```python\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "We'll use **Logistic Regression** and **Random Forest Classifier** to predict repeat purchases.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Random Forest Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "Assess how well your models perform using various metrics.\n",
    "\n",
    "### Predict on Test Data\n",
    "\n",
    "```python\n",
    "# Logistic Regression predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Random Forest predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "```\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "Understand precision, recall, f1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Logistic Regression Classification Report\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Random Forest Classification Report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Visualize the performance of your models.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Random Forest Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Accuracy Score\n",
    "\n",
    "Measure the overall correctness of the model.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression Accuracy\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f'Logistic Regression Accuracy: {accuracy_lr:.2f}')\n",
    "\n",
    "# Random Forest Accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimize your models by adjusting their parameters to achieve better performance.\n",
    "\n",
    "### Grid Search for Random Forest\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
    "```\n",
    "\n",
    "### Retrain with Best Parameters\n",
    "\n",
    "```python\n",
    "# Get the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Retrain the model\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "print(f'Optimized Random Forest Accuracy: {accuracy_best_rf:.2f}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "Make your model accessible for real-world use through a simple web application.\n",
    "\n",
    "### Save the Trained Model\n",
    "\n",
    "Use `joblib` to save your trained model for later use.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save the best Random Forest model\n",
    "joblib.dump(best_rf, 'best_random_forest_model.pkl')\n",
    "```\n",
    "\n",
    "### Create a Streamlit App\n",
    "\n",
    "Streamlit allows you to build interactive web applications for your models.\n",
    "\n",
    "1. **Install Streamlit**:\n",
    "\n",
    "    ```bash\n",
    "    pip install streamlit\n",
    "    ```\n",
    "\n",
    "2. **Create a New Python File**:\n",
    "\n",
    "    Create a file named `app.py` and add the following content:\n",
    "\n",
    "    ```python\n",
    "    import streamlit as st\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load('best_random_forest_model.pkl')\n",
    "\n",
    "    st.title('Retail Repeat Purchase Prediction App')\n",
    "\n",
    "    # Collect user input\n",
    "    recency = st.number_input('Recency (days since last purchase)', min_value=0, max_value=1000, value=30)\n",
    "    frequency = st.number_input('Frequency (total number of purchases)', min_value=0, max_value=1000, value=1)\n",
    "    monetary = st.number_input('Monetary (total spend)', min_value=0.0, max_value=100000.0, value=100.0)\n",
    "\n",
    "    # Prepare the input data for prediction\n",
    "    input_data = pd.DataFrame({\n",
    "        'Recency': [recency],\n",
    "        'Frequency': [frequency],\n",
    "        'Monetary': [monetary]\n",
    "    })\n",
    "\n",
    "    # Predict the outcome\n",
    "    if st.button('Predict'):\n",
    "        prediction = model.predict(input_data)\n",
    "        if prediction[0] == 1:\n",
    "            st.error('The model predicts that the customer will make a repeat purchase.')\n",
    "        else:\n",
    "            st.success('The model predicts that the customer will not make a repeat purchase.')\n",
    "    ```\n",
    "\n",
    "3. **Run the Streamlit App**:\n",
    "\n",
    "    In your terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    streamlit run app.py\n",
    "    ```\n",
    "\n",
    "    A new browser window will open displaying your web application.\n",
    "\n",
    "---\n",
    "\n",
    "## Documenting and Presenting Your Work\n",
    "\n",
    "Effective documentation and presentation are crucial for communicating your findings.\n",
    "\n",
    "### Write a Report\n",
    "\n",
    "Include the following sections in your report:\n",
    "\n",
    "- **Introduction**: Explain the problem and objectives.\n",
    "- **Data Exploration**: Summarize your data analysis.\n",
    "- **Data Cleaning and Preprocessing**: Describe the steps taken to prepare the data.\n",
    "- **Feature Engineering**: Detail how you transformed the data.\n",
    "- **Modeling**: Explain the models used and their performance.\n",
    "- **Conclusion**: Highlight key insights and recommendations.\n",
    "\n",
    "### Create Visualizations\n",
    "\n",
    "Use charts and graphs to illustrate your findings.\n",
    "\n",
    "- **RFM Distribution**: Shows the distribution of Recency, Frequency, and Monetary values.\n",
    "\n",
    "    ```python\n",
    "    # RFM Distribution\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.histplot(rfm['Recency'], bins=30, ax=axes[0], kde=True)\n",
    "    axes[0].set_title('Recency Distribution')\n",
    "\n",
    "    sns.histplot(rfm['Frequency'], bins=30, ax=axes[1], kde=True)\n",
    "    axes[1].set_title('Frequency Distribution')\n",
    "\n",
    "    sns.histplot(rfm['Monetary'], bins=30, ax=axes[2], kde=True)\n",
    "    axes[2].set_title('Monetary Distribution')\n",
    "\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "- **Feature Importance**: Displays which features are most influential in predicting repeat purchases.\n",
    "\n",
    "    ```python\n",
    "    # Feature Importance for Random Forest\n",
    "    importances = best_rf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "    plt.title('Feature Importances')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "### Prepare a Presentation\n",
    "\n",
    "Design slides to present your project to stakeholders.\n",
    "\n",
    "- **Slide 1**: Title and Objectives\n",
    "- **Slide 2**: Data Overview\n",
    "- **Slide 3**: Data Cleaning and Preprocessing\n",
    "- **Slide 4**: Feature Engineering\n",
    "- **Slide 5**: Modeling Approach and Results\n",
    "- **Slide 6**: Confusion Matrix and Performance Metrics\n",
    "- **Slide 7**: Feature Importance\n",
    "- **Slide 8**: Deployment and Application Demo\n",
    "- **Slide 9**: Conclusions and Recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Success\n",
    "\n",
    "- **Stay Organized**: Keep your code clean and well-commented.\n",
    "- **Understand Your Data**: Spend ample time exploring and understanding your dataset.\n",
    "- **Iterate**: Continuously refine your model based on evaluation metrics.\n",
    "- **Seek Feedback**: Share your progress with peers or mentors for constructive feedback.\n",
    "- **Manage Your Time**: Set deadlines for each project phase to stay on track.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've successfully completed the Retail Analytics project. This project not only enhances your data analysis and machine learning skills but also provides valuable insights into customer behavior and business operations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f5cc0-6e5d-4b28-befa-191d80afb61d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1a02fe7-6730-4110-a3d5-08e92ccdca1f",
   "metadata": {},
   "source": [
    "## Additional Teaching and Support Notes\n",
    "\n",
    "### Checkpoints and Milestones\n",
    "- Submit a proposal after dataset selection and problem definition.\n",
    "- Review processed datasets after ETL and feature engineering.\n",
    "- Get feedback on model performance before final submission.\n",
    "\n",
    "### Workshops\n",
    "- Topics include:\n",
    "  - Parsing messy datasets.\n",
    "  - Handling large text data efficiently.\n",
    "  - Creating deployable apps.\n",
    "\n",
    "### Common Pitfalls\n",
    "- Incomplete data cleaning causing errors.\n",
    "- Overlooking proper train/test splitting.\n",
    "- Ignoring problem-specific evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "By following these outlines, you will gain guidance on domain-specific considerations, practical suggestions for feature engineering, model selection tips, and storytelling best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
